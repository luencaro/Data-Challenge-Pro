{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11131163,"sourceType":"datasetVersion","datasetId":6942282}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importacion De Librerias y Datos**","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T02:50:54.432212Z","iopub.execute_input":"2025-04-25T02:50:54.432690Z","iopub.status.idle":"2025-04-25T02:50:54.437839Z","shell.execute_reply.started":"2025-04-25T02:50:54.432648Z","shell.execute_reply":"2025-04-25T02:50:54.436900Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# **Analisis Exploratorio De Datos (EDA)**","metadata":{}},{"cell_type":"code","source":"# Vista previa de cada dataset\ndf_salud = df.copy()\nprint(\"DF Salud:\")\ndisplay(df_salud.head())\nprint(\"dtypes:\")\nprint(df_salud.dtypes)\n\n\nprint(\"DF Prestadores:\")\ndisplay(df_prestadores.head())\nprint(\"dtypes:\")\nprint(df_prestadores.dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T04:26:15.316725Z","iopub.execute_input":"2025-04-25T04:26:15.317081Z","iopub.status.idle":"2025-04-25T04:26:16.634227Z","shell.execute_reply.started":"2025-04-25T04:26:15.317059Z","shell.execute_reply":"2025-04-25T04:26:16.633057Z"}},"outputs":[{"name":"stdout","text":"DF Salud:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       FECHA_ATENCION             Concepto_Factura_Desc  Cantidad  \\\n0 2019-07-12 06:20:00  MSI -MEDICO SEGUIMIENTO INTEGRAL       1.0   \n1 2019-06-18 11:08:00          SESIONES DE FISIOTERAPIA       2.0   \n2 2019-03-05 07:20:00              CONSULTA ORTOPEDISTA       1.0   \n3 2019-10-05 10:03:00            CONSULTA NO PROGRAMADA       1.0   \n4 2019-06-25 07:13:00                       RADIOGRAFIA       1.0   \n\n          MUNICIPIO  \n0            BOGOTA  \n1            BOGOTA  \n2            BOGOTA  \n3  SANTIAGO DE CALI  \n4           GUACARI  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FECHA_ATENCION</th>\n      <th>Concepto_Factura_Desc</th>\n      <th>Cantidad</th>\n      <th>MUNICIPIO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-07-12 06:20:00</td>\n      <td>MSI -MEDICO SEGUIMIENTO INTEGRAL</td>\n      <td>1.0</td>\n      <td>BOGOTA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-06-18 11:08:00</td>\n      <td>SESIONES DE FISIOTERAPIA</td>\n      <td>2.0</td>\n      <td>BOGOTA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-03-05 07:20:00</td>\n      <td>CONSULTA ORTOPEDISTA</td>\n      <td>1.0</td>\n      <td>BOGOTA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-10-05 10:03:00</td>\n      <td>CONSULTA NO PROGRAMADA</td>\n      <td>1.0</td>\n      <td>SANTIAGO DE CALI</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-06-25 07:13:00</td>\n      <td>RADIOGRAFIA</td>\n      <td>1.0</td>\n      <td>GUACARI</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"dtypes:\nFECHA_ATENCION           datetime64[ns]\nConcepto_Factura_Desc            object\nCantidad                        float64\nMUNICIPIO                        object\ndtype: object\nDF Prestadores:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-fa8a899bab2e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DF Prestadores:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_prestadores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtypes:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_prestadores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_prestadores' is not defined"],"ename":"NameError","evalue":"name 'df_prestadores' is not defined","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"# Información general de los DataFrame\nprint(\"Información de df_salud:\")\ndf_salud.info()\n\nprint(\"Informacion de Prestadores\")\ndf_prestadores.info()\n\n# Estadísticas descriptivas (numéricas y algunas categóricas)\nprint(\"Estadísticas descriptivas de df_salud:\")\ndisplay(df_salud.describe(include='all'))\n\nprint(\"Estadísticas descriptivas de df_salud:\")\ndisplay(df_prestadores.describe(include='all'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Analisis de Valores Nulos Y Duplicados**","metadata":{}},{"cell_type":"code","source":"# Valores nulos\nprint(\"Valores nulos de datos de Salud\")\ndf_salud.isnull().sum()\nprint(\"Valores nulos de datos de Prestadores\")\ndf_prestadores.isnull().sum","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Duplicados de datos de Salud\")\ndf_salud.duplicated()\nprint(\"DUplicados de datos de Prestadores\")\ndf_prestadores.duplicated()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Estadísticas de capacidad máxima\ndisplay(df_prestadores[\"max_cantidad\"].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Tipos de Atencion Medica mas Comunes**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Identifica las 10 etiquetas más comunes\ntop10 = df_salud[\"Concepto_Factura_Desc\"].value_counts().nlargest(10).index\n\n# Filtra el DataFrame para quedarte sólo con esas 10\ndf_top10 = df_salud[df_salud[\"Concepto_Factura_Desc\"].isin(top10)]\n\n# Grafica de barras\nplt.figure(figsize=(10, 6))\nsns.countplot(\n    data=df_top10,\n    y=\"Concepto_Factura_Desc\",\n    order=top10\n)\nplt.title(\"Top 10 Tipos de Atención Médica\")\nplt.xlabel(\"Número de facturas\")\nplt.ylabel(\"Concepto de Factura\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Distribucion de categorias de CIE-10**","metadata":{}},{"cell_type":"code","source":"# Agrupar códigos CIE-10 por categoría (ejemplo: primera letra)\ndf_salud[\"Categoria_CIE10\"] = df_salud[\"Siniestro_Diagnosti_Princi_Id\"].str[0]\n\n# Gráfico de barras con escala logarítmica\nplt.figure(figsize=(12, 6))\nax = sns.countplot(data=df_salud, x=\"Categoria_CIE10\", order=df_salud[\"Categoria_CIE10\"].value_counts().index)\n\n# Aplicar escala logarítmica al eje Y\nplt.yscale('log')\n\n# Añadir título y etiquetas\nplt.title(\"Distribución de Categorías CIE-10 (Escala Logarítmica)\")\nplt.xlabel(\"Categorías CIE-10\")\nplt.ylabel(\"Conteo (escala logarítmica)\")\n\n# Mejorar la visualización\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Asegurarse de que FECHA_ATENCION es datetime\ndf_salud['Mes'] = df_salud['FECHA_ATENCION'].dt.to_period('M')\n\n# Agrupar la demanda (Cantidad) por mes\ndemanda_por_mes = df_salud.groupby('Mes')['Cantidad'].sum().sort_index()\n\nplt.figure(figsize=(18,6))\ndemanda_por_mes.plot(marker='o')\nplt.title(\"Demanda Mensual Total\")\nplt.xlabel(\"Mes\")\nplt.ylabel(\"Cantidad\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Agrupar y sumar las cantidades por municipio\ndemanda_municipio = (\n    df_salud\n    .groupby(\"MUNICIPIO\", observed=True)[\"Cantidad\"]\n    .sum()\n    .reset_index()\n)\n\n# Obtener los 10 municipios con mayor demanda\ntop10 = demanda_municipio.nlargest(10, \"Cantidad\")\n\n# Ordenar los valores para el gráfico (opcional, dependiendo de la preferencia)\ntop10 = top10.sort_values(\"Cantidad\", ascending=True)\n\n# Crear el gráfico\nplt.figure(figsize=(12, 6))\nsns.barplot(\n    data=top10,\n    x=\"Cantidad\",\n    y=\"MUNICIPIO\",\n    order=top10[\"MUNICIPIO\"]  # Asegura el orden correcto\n)\nplt.title(\"Municipios con Mayor Demanda de Servicios\")\nplt.tight_layout()  # Mejora el espacio del gráfico\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Tendencias por tipo de servicio médico**","metadata":{}},{"cell_type":"code","source":"# Asegurarse de que FECHA_ATENCION es datetime\ndf_salud['Mes'] = df_salud['FECHA_ATENCION'].dt.to_period('M')\n\n# Agrupar por mes y tipo de atención\ntendencia_por_tipo = df_salud.groupby(['Mes', 'Concepto_Factura_Desc'])['Cantidad'].sum().reset_index()\n\n# Obtener los 10 tipos de atención más comunes para simplificar la visualización\ntop_tipos = df_salud['Concepto_Factura_Desc'].value_counts().nlargest(10).index\n\n# Filtrar solo los tipos top\ntendencia_por_tipo_top = tendencia_por_tipo[tendencia_por_tipo['Concepto_Factura_Desc'].isin(top_tipos)]\n\n# Convertir Mes a string para mejor visualización\ntendencia_por_tipo_top['Mes'] = tendencia_por_tipo_top['Mes'].astype(str)\n\n# Crear gráfico de líneas\nplt.figure(figsize=(16, 8))\nfor tipo in top_tipos:\n    data = tendencia_por_tipo_top[tendencia_por_tipo_top['Concepto_Factura_Desc'] == tipo]\n    plt.plot(data['Mes'], data['Cantidad'], marker='o', label=tipo)\n\nplt.title('Tendencia de Demanda por Tipo de Servicio Médico')\nplt.xlabel('Mes')\nplt.ylabel('Cantidad')\nplt.legend()\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Análisis específico por municipio**","metadata":{}},{"cell_type":"code","source":"# Agrupar datos por municipio\ndatos_por_municipio = df_salud.groupby('MUNICIPIO').agg({\n    'Cantidad': 'sum',\n    'FECHA_ATENCION': 'count',\n    'Concepto_Factura_Desc': lambda x: x.nunique(),\n    'TIPIFICACION': lambda x: x.nunique()\n}).reset_index()\n\n# Renombrar columnas\ndatos_por_municipio.columns = ['Municipio', 'Total_Demanda', 'Num_Atenciones', 'Tipos_Servicio', 'Tipificaciones']\n\n# Calcular promedio de demanda por atención\ndatos_por_municipio['Demanda_Promedio'] = datos_por_municipio['Total_Demanda'] / datos_por_municipio['Num_Atenciones']\n\n# Visualizar estadísticas por municipio (todos los municipios)\nprint(\"Estadísticas por municipio:\")\ndisplay(datos_por_municipio.sort_values('Total_Demanda', ascending=False))\n\n# Analizar más a fondo los 10 municipios principales\ntop10_municipios = datos_por_municipio.nlargest(10, 'Total_Demanda')['Municipio'].tolist()\n\n# Para cada municipio top, analizar la tendencia mensual\nplt.figure(figsize=(15, 10))\n\nfor i, municipio in enumerate(top10_municipios):\n    # Filtrar datos del municipio\n    df_muni = df_salud[df_salud['MUNICIPIO'] == municipio]\n    \n    # Agrupar por mes\n    df_muni_mes = df_muni.groupby(df_muni['FECHA_ATENCION'].dt.to_period('M'))['Cantidad'].sum()\n    \n    # Crear subgráfico\n    plt.subplot(5, 2, i+1)\n    df_muni_mes.plot()\n    plt.title(f'Tendencia mensual: {municipio}')\n    plt.xlabel('Mes')\n    plt.ylabel('Cantidad')\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Patrones estacionales**","metadata":{}},{"cell_type":"code","source":"!pip install statsmodels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Crear variables para diferentes medidas temporales\ndf_salud['Año'] = df_salud['FECHA_ATENCION'].dt.year\ndf_salud['Mes'] = df_salud['FECHA_ATENCION'].dt.month\ndf_salud['DiaSemana'] = df_salud['FECHA_ATENCION'].dt.dayofweek  # 0=Lunes, 6=Domingo\ndf_salud['DiaMes'] = df_salud['FECHA_ATENCION'].dt.day\ndf_salud['Trimestre'] = df_salud['FECHA_ATENCION'].dt.quarter\n\n# 1. Análisis por mes del año\ndemanda_por_mes = df_salud.groupby('Mes')['Cantidad'].sum().reset_index()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Mes', y='Cantidad', data=demanda_por_mes)\nplt.title('Demanda por Mes del Año')\nplt.xlabel('Mes')\nplt.ylabel('Cantidad')\nplt.xticks(range(12), ['Ene', 'Feb', 'Mar', 'Abr', 'May', 'Jun', 'Jul', 'Ago', 'Sep', 'Oct', 'Nov', 'Dic'])\nplt.show()\n\n# Análisis por día de la semana\ndemanda_por_dia = df_salud.groupby('DiaSemana')['Cantidad'].sum().reset_index()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='DiaSemana', y='Cantidad', data=demanda_por_dia)\nplt.title('Demanda por Día de la Semana')\nplt.xlabel('Día de la Semana')\nplt.ylabel('Cantidad')\nplt.xticks(range(7), ['Lun', 'Mar', 'Mié', 'Jue', 'Vie', 'Sáb', 'Dom'])\nplt.show()\n\n# Análisis por trimestre\ndemanda_por_trimestre = df_salud.groupby(['Año', 'Trimestre'])['Cantidad'].sum().reset_index()\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(x='Trimestre', y='Cantidad', hue='Año', data=demanda_por_trimestre, marker='o')\nplt.title('Demanda Trimestral por Año')\nplt.xlabel('Trimestre')\nplt.ylabel('Cantidad')\nplt.grid(True)\nplt.show()\n\n\n# Análisis de estacionalidad \nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Preparar serie temporal para decomposición\nts_data = df_salud.groupby(df_salud['FECHA_ATENCION'].dt.to_period('M'))['Cantidad'].sum()\nts_data.index = pd.to_datetime(ts_data.index.astype(str))\n\n# Realizar descomposición estacional (si hay suficientes datos)\nif len(ts_data) >= 24:  # Necesitamos al menos 2 años de datos\n    decomposition = seasonal_decompose(ts_data, model='additive', period=12)\n    \n    # Graficar la descomposición\n    plt.figure(figsize=(12, 10))\n    plt.subplot(411)\n    plt.plot(ts_data, label='Original')\n    plt.legend(loc='best')\n    plt.title('Descomposición Estacional')\n    \n    plt.subplot(412)\n    plt.plot(decomposition.trend, label='Tendencia')\n    plt.legend(loc='best')\n    \n    plt.subplot(413)\n    plt.plot(decomposition.seasonal, label='Estacionalidad')\n    plt.legend(loc='best')\n    \n    plt.subplot(414)\n    plt.plot(decomposition.resid, label='Residuos')\n    plt.legend(loc='best')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Detección de valores atípicos, datos faltantes y errores**","metadata":{}},{"cell_type":"code","source":"# Análisis de valores faltantes\nprint(\"Total de valores faltantes por columna:\")\nprint(df_salud.isnull().sum())\n\n# Visualización de valores faltantes\nplt.figure(figsize=(12, 6))\nsns.heatmap(df_salud.isnull(), cbar=False, yticklabels=False, cmap='viridis')\nplt.title('Mapa de Valores Faltantes')\nplt.show()\n\n# Detección de valores atípicos en la variable \"Cantidad\"\nplt.figure(figsize=(10, 6))\nsns.boxplot(y=df_salud['Cantidad'])\nplt.title('Distribución de la Variable Cantidad')\nplt.ylabel('Cantidad')\nplt.show()\n\n# Cálculo de percentiles para identificar outliers\nQ1 = df_salud['Cantidad'].quantile(0.25)\nQ3 = df_salud['Cantidad'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\nprint(f\"Valores atípicos - Límite inferior: {lower_bound}, Límite superior: {upper_bound}\")\nprint(f\"Número de outliers: {df_salud[(df_salud['Cantidad'] < lower_bound) | (df_salud['Cantidad'] > upper_bound)].shape[0]}\")\n\n# Análisis de valores inconsistentes\n# Verificar si hay cantidades negativas o cero (que podrían ser erróneas)\nprint(f\"Registros con cantidad negativa: {df_salud[df_salud['Cantidad'] < 0].shape[0]}\")\nprint(f\"Registros con cantidad cero: {df_salud[df_salud['Cantidad'] == 0].shape[0]}\")\n\n# Verificación de fechas\nmin_date = df_salud['FECHA_ATENCION'].min()\nmax_date = df_salud['FECHA_ATENCION'].max()\nprint(f\"Rango de fechas: {min_date} a {max_date}\")\n\n# Comprobar si hay fechas futuras (posteriores a la fecha actual)\nfuture_dates = df_salud[df_salud['FECHA_ATENCION'] > pd.Timestamp.now()]\nprint(f\"Registros con fechas futuras: {future_dates.shape[0]}\")\n\n# Verificar duplicados exactos\nduplicados = df_salud.duplicated()\nprint(f\"Número de registros duplicados: {duplicados.sum()}\")\n\n# Comprobar problemas potenciales en los valores categóricos\nfor col in ['Concepto_Factura_Desc', 'MUNICIPIO']:\n    n_values = df_salud[col].nunique()\n    print(f\"Columna {col}: {n_values} valores únicos\")\n    \n    # Si hay muchos valores únicos, puede haber inconsistencias\n    if n_values > 100:  # Umbral arbitrario\n        print(\"  ⚠️ Esta columna tiene muchos valores únicos, posible inconsistencia en nomenclatura\")\n        \n        # Mostrar los valores más y menos frecuentes\n        print(\"  Valores más comunes:\")\n        print(df_salud[col].value_counts().head())\n        print(\"  Valores menos comunes:\")\n        print(df_salud[col].value_counts().tail())\n\n# Análisis de la capacidad máxima del prestador vs. demanda real\nif 'Geogra_Municipio_Id' in df_salud.columns and 'Geogra_Municipio_Id' in df_prestadores.columns:\n    # Agrupar demanda por municipio\n    demanda_municipio = df_salud.groupby('Geogra_Municipio_Id')['Cantidad'].sum().reset_index()\n    \n    # Combinar con información de capacidad\n    analisis_capacidad = demanda_municipio.merge(df_prestadores, on='Geogra_Municipio_Id', how='inner')\n    analisis_capacidad['Porcentaje_Uso'] = (analisis_capacidad['Cantidad'] / analisis_capacidad['max_cantidad']) * 100\n    \n    # Identificar posibles problemas\n    print(\"Municipios con posible sobrecarga (uso > 90%):\")\n    display(analisis_capacidad[analisis_capacidad['Porcentaje_Uso'] > 90])\n    \n    print(\"Municipios con baja utilización (uso < 20%):\")\n    display(analisis_capacidad[analisis_capacidad['Porcentaje_Uso'] < 20])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Análisis de series temporales específico**","metadata":{}},{"cell_type":"code","source":"import matplotlib.dates as mdates\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nimport statsmodels.api as sm\n\n# Preparar serie temporal diaria\nts_diaria = df_salud.groupby(df_salud['FECHA_ATENCION'].dt.date)['Cantidad'].sum()\nts_diaria.index = pd.to_datetime(ts_diaria.index)\n\n# Visualizar la serie temporal completa\nplt.figure(figsize=(15, 7))\nplt.plot(ts_diaria)\nplt.title('Serie Temporal Diaria de Demanda')\nplt.xlabel('Fecha')\nplt.ylabel('Cantidad')\nplt.grid(True)\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\nplt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Analizar la estacionariedad de la serie\nresult = adfuller(ts_diaria.dropna())\nprint('Prueba de Dickey-Fuller Aumentada:')\nprint(f'Estadística de prueba: {result[0]}')\nprint(f'Valor p: {result[1]}')\nprint(f'Valores críticos: {result[4]}')\nif result[1] <= 0.05:\n    print(\"La serie es estacionaria (rechazamos H0)\")\nelse:\n    print(\"La serie no es estacionaria (no rechazamos H0)\")\n\n# Analizar autocorrelación\nplt.figure(figsize=(12, 10))\n\n# ACF\nplt.subplot(211)\nsm.graphics.tsa.plot_acf(ts_diaria.dropna(), lags=40, ax=plt.gca())\nplt.title('Función de Autocorrelación (ACF)')\n\n# PACF\nplt.subplot(212)\nsm.graphics.tsa.plot_pacf(ts_diaria.dropna(), lags=40, ax=plt.gca())\nplt.title('Función de Autocorrelación Parcial (PACF)')\n\nplt.tight_layout()\nplt.show()\n\n# Descomposición de la serie temporal (tendencia, estacionalidad, residuos)\n# Usar la serie semanal para reducir el ruido\nts_semanal = df_salud.groupby(pd.Grouper(key='FECHA_ATENCION', freq='W'))['Cantidad'].sum()\n\n# Realizar descomposición\ndescomposicion = sm.tsa.seasonal_decompose(ts_semanal, model='additive', period=52)\n\n# Graficar\nfig = descomposicion.plot()\nfig.set_size_inches(15, 12)\nplt.tight_layout()\nplt.show()\n\n# Análisis de venta móvil para detectar tendencias a largo plazo\nventanas = [7, 30, 90, 180]\nplt.figure(figsize=(15, 8))\n\nplt.plot(ts_diaria, alpha=0.5, label='Original')\n\nfor ventana in ventanas:\n    ts_ma = ts_diaria.rolling(window=ventana).mean()\n    plt.plot(ts_ma, label=f'Media Móvil ({ventana} días)')\n\nplt.title('Análisis de Tendencia con Medias Móviles')\nplt.xlabel('Fecha')\nplt.ylabel('Cantidad')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Ingenieria de Caractristicas**","metadata":{}},{"cell_type":"code","source":"# Feature Engineering consolidado\ndef create_features(df):\n    \"\"\"Función unificada de ingeniería de características.\"\"\"\n    print(\"Creando características...\")\n    \n    # 1. Características temporales\n    df['year'] = df['FECHA_ATENCION'].dt.year\n    df['month'] = df['FECHA_ATENCION'].dt.month\n    df['day'] = df['FECHA_ATENCION'].dt.day\n    df['dayofweek'] = df['FECHA_ATENCION'].dt.dayofweek\n    df['quarter'] = df['FECHA_ATENCION'].dt.quarter\n    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n    \n    # Variables cíclicas (mejor representación de temporalidad)\n    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)\n    \n    # Periodo COVID\n    df['covid_period'] = (((df['year'] == 2020) & (df['month'] >= 3)) | (df['year'] == 2021)).astype(int)\n    \n    # 2. Características de lag y ventana por municipio y tipo de servicio\n    print(\"Creando características de lag y ventana móvil...\")\n    id_columns = ['MUNICIPIO', 'Concepto_Factura_Desc']\n    \n    # Ordenar por grupos y fecha para cálculo de lags\n    df = df.sort_values(id_columns + ['FECHA_ATENCION'])\n    \n    # Crear características agregadas por grupo\n    groups = df.groupby(id_columns)\n    \n    # Columnas para almacenar lags\n    lag_features = ['lag_7d', 'lag_30d', 'rolling_mean_7d', 'rolling_mean_30d']\n    for feature in lag_features:\n        df[feature] = np.nan\n    \n    # Calcular lags por grupo (municipio y tipo de servicio)\n    for name, group in groups:\n        indices = group.index\n        \n        # Lags básicos\n        df.loc[indices, 'lag_7d'] = group['Cantidad'].shift(7)\n        df.loc[indices, 'lag_30d'] = group['Cantidad'].shift(30)\n        \n        # Características de ventana móvil (medias)\n        df.loc[indices, 'rolling_mean_7d'] = group['Cantidad'].rolling(window=7).mean().shift(1)\n        df.loc[indices, 'rolling_mean_30d'] = group['Cantidad'].rolling(window=30).mean().shift(1)\n    \n    # 3. Características estadísticas agregadas\n    print(\"Creando características estadísticas...\")\n    \n    # Estadísticas por municipio\n    muni_stats = df.groupby('MUNICIPIO')['Cantidad'].agg(['mean', 'median']).reset_index()\n    muni_stats.columns = ['MUNICIPIO', 'municipio_mean', 'municipio_median']\n    \n    # Estadísticas por tipo de servicio\n    service_stats = df.groupby('Concepto_Factura_Desc')['Cantidad'].agg(['mean', 'median']).reset_index()\n    service_stats.columns = ['Concepto_Factura_Desc', 'service_mean', 'service_median']\n    \n    # Hacer merge de estadísticas al dataframe principal\n    df = pd.merge(df, muni_stats, on='MUNICIPIO', how='left')\n    df = pd.merge(df, service_stats, on='Concepto_Factura_Desc', how='left')\n    \n    # Características relativas\n    df['cantidad_vs_muni_avg'] = df['Cantidad'] / df['municipio_mean']\n    df['cantidad_vs_service_avg'] = df['Cantidad'] / df['service_mean']\n    \n    # Eliminar filas con NaN debido a los lags\n    df = df.dropna()\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Función para codificar variables categóricas \n\ncc = ['TIPIFICACION', 'Siniestro_Diagnosti_Princi_Id', 'MUNICIPIO', 'Geogra_Municipio_Id', 'Concepto_Factura_Desc', 'Nombre_Tipo_Atencion_Arp']\ndef encode_categorical(df, categorical_cols=cc):\n    \"\"\"Codificación eficiente de variables categóricas.\"\"\"\n    print(\"Codificando variables categóricas...\")\n    \n    # Para cada columna categórica, calcular la media del target por grupo\n    for col in categorical_cols:\n        # Calcular la media global (para usar como prior)\n        global_mean = df['Cantidad'].mean()\n        \n        # Calcular codificación por grupo\n        means = df.groupby(col)['Cantidad'].mean()\n        \n        # Crear la columna codificada (con regularización)\n        df[f'{col}_encoded'] = df[col].map(means).fillna(global_mean)\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def select_features(X, y, n_features=30):\n    \"\"\"\n    Implementa varios métodos de selección de características y devuelve\n    las características seleccionadas por cada método\n    \"\"\"\n    # Asegurar que solo se utilizan columnas numéricas\n    numeric_cols = X.select_dtypes(include=['number']).columns\n    X = X[numeric_cols]\n    \n    features = X.columns.tolist()\n    results = {}\n    \n    # 1. Selección basada en correlación (valor absoluto)\n    corr_vals = X.corrwith(y).abs()\n    selected_corr = corr_vals.sort_values(ascending=False).index[:n_features].tolist()\n    results['correlation'] = selected_corr\n    \n    # 2. Selección basada en Random Forest\n    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n    rf.fit(X, y)\n    importance = pd.Series(rf.feature_importances_, index=features)\n    selected_rf = importance.sort_values(ascending=False).index[:n_features].tolist()\n    results['random_forest'] = selected_rf\n    \n    # 3. Recursive Feature Elimination (RFE)\n    from sklearn.feature_selection import RFE\n    rfe = RFE(estimator=RandomForestRegressor(n_estimators=50, random_state=42), \n              n_features_to_select=n_features)\n    rfe.fit(X, y)\n    selected_rfe = [features[i] for i in range(len(features)) if rfe.support_[i]]\n    results['rfe'] = selected_rfe\n    \n    # 4. Selección basada en información mutua\n    mi = mutual_info_regression(X, y)\n    mi_series = pd.Series(mi, index=features)\n    selected_mi = mi_series.sort_values(ascending=False).index[:n_features].tolist()\n    results['mutual_info'] = selected_mi\n    \n    # Identificar características comunes entre métodos\n    common_features = set(selected_corr)\n    for method in ['random_forest', 'rfe', 'mutual_info']:\n        common_features = common_features.intersection(set(results[method]))\n    \n    results['common'] = list(common_features)\n    \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_features = create_features(df_salud)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_encoded = encode_categorical(df_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Selección de características\nX = df_encoded.drop(['Cantidad', 'FECHA_ATENCION'], axis=1)\ny = df_encoded['Cantidad']\n\n# Ejecutar la selección de características\n#selected_features = select_features(X, y, n_features=30)\n\nprint(selected_features)\n# Crear un DataFrame con las características seleccionadas\n#X_selected = X[selected_features['common']] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(X_selected.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **HERE**","metadata":{}},{"cell_type":"markdown","source":"# **THERE**","metadata":{}},{"cell_type":"code","source":"# forecasting_pipeline.py\n# Pipeline avanzado: forecasting 365 días con nested CV, early stopping, SMAPE/MASE y retrain final sobre N-Beats\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom statsmodels.tsa.arima.model import ARIMA\nimport xgboost as xgb\nimport holidays\nfrom sklearn.model_selection import TimeSeriesSplit, ParameterGrid\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# ------------------------- Configuración -------------------------------\nGPUS = tf.config.list_physical_devices('GPU')\nif GPUS:\n    for g in GPUS:\n        tf.config.experimental.set_memory_growth(g, True)\n    print(f\"GPUs detectadas: {len(GPUS)}\")\nelse:\n    print(\"Usando CPU\")\n\n# Parámetros\nXGB_PARAMS = {'tree_method':'gpu_hist','predictor':'gpu_predictor','verbosity':0}\nWINDOW = 30\nTOP_N = 5\nTOP_M = 3\nCV_SPLITS = 5\n\n# ------------------------- Métricas extendidas ---------------------------\ndef smape(y_true, y_pred):\n    denom = (np.abs(y_true) + np.abs(y_pred)) / 2\n    mask = denom != 0\n    return 100 * np.mean(np.abs(y_true[mask] - y_pred[mask]) / denom[mask])\n\ndef mase(y_true, y_pred, y_insample, season=1):\n    mae_model = mean_absolute_error(y_true, y_pred)\n    diff = np.abs(y_insample[season:] - y_insample[:-season])\n    mae_naive = diff.mean()\n    return mae_model / mae_naive if mae_naive != 0 else np.nan\n\n# ------------------------- Data Prep ---------------------------------------\ndef load_data(path):\n    df = pd.read_csv(path, sep='|', usecols=['FECHA_ATENCION','Concepto_Factura_Desc','Cantidad','MUNICIPIO'], parse_dates=['FECHA_ATENCION'])\n    df.drop_duplicates(inplace=True)\n    df = df[df['Cantidad'].notna()]\n    df['MUNICIPIO'] = df['MUNICIPIO'].str.split('-').str[0].str.strip()\n    return df\n\n# Exógenas\ndef make_exog(idx):\n    cal = holidays.Colombia()\n    df = pd.DataFrame(index=idx)\n    df['is_weekend'] = (idx.dayofweek >= 5).astype(int)\n    df['is_holiday'] = idx.to_series().apply(lambda d: 1 if d in cal else 0)\n    df['covid'] = idx.to_series().apply(lambda d: 1 if (d.year==2020 and d.month>=3) or d.year==2021 else 0)\n    return df\n\n# Serie diaria univariante\ndef get_ts(df, muni, serv):\n    ts = df[(df['MUNICIPIO']==muni)&(df['Concepto_Factura_Desc']==serv)]\n    ts = ts.set_index('FECHA_ATENCION')['Cantidad']\n    ts = ts.resample('D').sum().asfreq('D').fillna(0)\n    return ts\n\n# Features: lags + exógenas\ndef make_features(ts):\n    ex = make_exog(ts.index)\n    df = ex.copy(); df['y'] = ts\n    for L in [7, 30]: df[f'lag_{L}'] = df['y'].shift(L)\n    df['roll7'] = df['y'].rolling(7).mean().shift(1)\n    df.dropna(inplace=True)\n    return df.drop('y',axis=1), df['y']\n\n# Secuencias para DL modelo\ndef make_seq(vals, w):\n    X, y = [], []\n    for i in range(len(vals) - w): X.append(vals[i:i+w]); y.append(vals[i+w])\n    return np.array(X), np.array(y)\n\n# ------------------------- Train/Evaluate CV -------------------------------\ndef train_evaluate(ts, model_type, params):\n    tscv = TimeSeriesSplit(n_splits=CV_SPLITS)\n    scores = []\n    y_insample = ts.values\n    for tr, te in tscv.split(ts):\n        t_tr, t_te = ts.iloc[tr], ts.iloc[te]\n        if len(t_tr) < WINDOW + 1 or len(t_te) == 0: continue\n        # Combined: ARIMA + XGB\n        if model_type == 'Combined':\n            ar = ARIMA(t_tr, order=(1,1,1)).fit()\n            resid = (t_tr - ar.fittedvalues).dropna()\n            Xr, yr = make_features(resid)\n            xbm = xgb.XGBRegressor(**params).fit(Xr, yr)\n            fc_ar = ar.forecast(steps=len(t_te))\n            hist = pd.concat([resid, pd.Series(fc_ar, index=t_te.index)])\n            Xf, _ = make_features(hist); Xf = Xf.loc[t_te.index]\n            fc_xgb = xbm.predict(Xf)\n            y_pred = fc_ar.values + fc_xgb; y_true = t_te.values\n        # LSTM\n        elif model_type == 'LSTM':\n            Xtr, Ytr = make_seq(t_tr.values, WINDOW)\n            Xte, _ = make_seq(np.concatenate([t_tr.values[-WINDOW:], t_te.values]), WINDOW)\n            model = models.Sequential([\n                layers.Input((WINDOW,1)), layers.LSTM(params['units']), layers.Dropout(0.2), layers.Dense(1)\n            ])\n            model.compile('adam','mse')\n            es = callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n            model.fit(Xtr.reshape(-1,WINDOW,1), Ytr,\n                      epochs=params['epochs'], batch_size=32, validation_split=0.1,\n                      verbose=0, callbacks=[es])\n            y_pred = model.predict(Xte.reshape(-1,WINDOW,1), verbose=0).flatten()[-len(t_te):]; y_true = t_te.values\n        # NBeats\n        else:\n            Xtr, Ytr = make_seq(t_tr.values, WINDOW)\n            Xte, _ = make_seq(np.concatenate([t_tr.values[-WINDOW:], t_te.values]), WINDOW)\n            class Block(layers.Layer):\n                def __init__(self): \n                    super().__init__();\n                    self.hidden=[layers.Dense(params['units'],activation='relu') for _ in range(4)];\n                    self.backcast=layers.Dense(WINDOW); self.forecast=layers.Dense(1)\n                def call(self,x):\n                    h=x\n                    for lyr in self.hidden: h=lyr(h)\n                    return self.backcast(h), self.forecast(h)\n            inp=layers.Input(shape=(WINDOW,)); res=inp; fc=0\n            for _ in range(params['stacks']): b,f=Block()(res); res-=b; fc+=f\n            nb= models.Model(inp, fc); nb.compile('adam','mse')\n            es=callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n            nb.fit(Xtr, Ytr, epochs=params['epochs'], batch_size=32, validation_split=0.1,\n                   verbose=0, callbacks=[es])\n            y_pred = nb.predict(Xte, verbose=0).flatten()[-len(t_te):]; y_true = t_te.values\n        rmse = mean_squared_error(y_true, y_pred, squared=False)\n        mae  = mean_absolute_error(y_true, y_pred)\n        sm   = smape(y_true, y_pred)\n        mz   = mase(y_true, y_pred, y_insample)\n        scores.append({'RMSE':rmse,'MAE':mae,'SMAPE':sm,'MASE':mz})\n    return pd.DataFrame(scores).mean() if scores else pd.Series({'RMSE':np.nan,'MAE':np.nan,'SMAPE':np.nan,'MASE':np.nan})\n\n# Nested CV tuning NBeats\ndef nested_cv(ts, grid):\n    outer = TimeSeriesSplit(n_splits=3)\n    best_p, best_s = None, np.inf\n    for tr, _ in outer.split(ts):\n        t_tr = ts.iloc[tr]\n        for p in ParameterGrid(grid):\n            r = train_evaluate(t_tr, 'NBeats', p)['RMSE']\n            if r < best_s: best_s, best_p = r, p\n    return best_p, best_s\n\n# ------------------------- Ejecución Principal ----------------------------\nif __name__=='__main__':\n    df = load_data('/kaggle/input/data-sura/1.Informacion Salud 2019-2024-001.txt')\n    top_munis = df.groupby('MUNICIPIO')['Cantidad'].sum().nlargest(TOP_N).index\n    combos = [(m, s) for m in top_munis for s in df[df['MUNICIPIO']==m]\n              .groupby('Concepto_Factura_Desc')['Cantidad'].sum().nlargest(TOP_M).index]\n    print(f\"Procesando {len(combos)} combos\")\n\n    summary = []\n    for model_type in ['Combined','LSTM','NBeats']:\n        if model_type=='Combined': best_p = XGB_PARAMS\n        elif model_type=='LSTM': best_p = {'epochs':10,'units':64}\n        else: best_p, _ = nested_cv(get_ts(df,*combos[0]), {'epochs':[10,20],'stacks':[2,3],'units':[128,256]})\n        metrics_list = [train_evaluate(get_ts(df,m,s),model_type,best_p)\n                        for m,s in combos if len(get_ts(df,m,s))>WINDOW*2]\n        avg = pd.DataFrame(metrics_list).mean().to_dict(); avg['model']=model_type\n        summary.append(avg)\n    print(pd.DataFrame(summary))\n\n    best_model = min(summary, key=lambda x: x['RMSE'])['model']\n    print(f\"Mejor modelo: {best_model}\")\n    print(\"Pipeline completado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:26:06.262605Z","iopub.execute_input":"2025-04-25T03:26:06.262951Z","iopub.status.idle":"2025-04-25T04:01:15.552979Z","shell.execute_reply.started":"2025-04-25T03:26:06.262924Z","shell.execute_reply":"2025-04-25T04:01:15.551858Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"GPUs detectadas: 2\nProcesando 15 combos\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"        RMSE        MAE       SMAPE      MASE     model\n0  47.542680  39.009640   74.157574  0.766604  Combined\n1  86.030305  71.285107  114.018633  1.288490      LSTM\n2  36.229163  25.357210   57.830436  0.518143    NBeats\nMejor modelo: NBeats\nPipeline completado.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from forecasting_pipeline import df, combos, best_model, params_dict, plot_forecasts\n\ndef plot_metrics(summary_df):\n    \"\"\"\n    Dibuja comparación de RMSE, MAE, SMAPE y MASE por modelo.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    viz = summary_df.set_index('model')\n    metrics = ['RMSE','MAE','SMAPE','MASE']\n    ax = viz[metrics].plot(kind='bar', figsize=(10,6))\n    ax.set_title('Comparación de métricas por modelo')\n    ax.set_ylabel('Valor de la métrica')\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_forecasts(df, combos, model_type, params, horizon=HORIZON, n_plots=5):\n    \"\"\"\n    Grafica pronósticos frente a realidad para los primeros n_plots combos.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    selected = combos[:n_plots]\n    fig, axes = plt.subplots(n_plots, 1, figsize=(10, 4*n_plots))\n    for ax, (muni, serv) in zip(axes, selected):\n        ts = get_ts(df, muni, serv)\n        train, test = ts[:-horizon], ts[-horizon:]\n        # Entrena y predice\n        if model_type == 'Combined':\n            ar = ARIMA(train, order=(1,1,1)).fit()\n            resid = (train - ar.fittedvalues).dropna()\n            Xr, yr = make_features(resid)\n            xbm = xgb.XGBRegressor(**params).fit(Xr, yr)\n            fc_ar = ar.forecast(steps=horizon)\n            hist = pd.concat([resid, pd.Series(fc_ar, index=test.index)])\n            Xf, _ = make_features(hist)\n            Xf = Xf.loc[test.index]\n            fc_xgb = xbm.predict(Xf)\n            pred = fc_ar + fc_xgb\n        elif model_type == 'LSTM':\n            Xtr, Ytr = make_seq(train.values, WINDOW)\n            model = models.Sequential([\n                layers.Input((WINDOW,1)),\n                layers.LSTM(params['units']),\n                layers.Dropout(0.2),\n                layers.Dense(1)\n            ])\n            model.compile('adam','mse')\n            model.fit(\n                Xtr.reshape(-1,WINDOW,1), Ytr,\n                epochs=params['epochs'], batch_size=32,\n                verbose=0\n            )\n            seq = np.concatenate([train.values[-WINDOW:], test.values])\n            Xf, _ = make_seq(seq, WINDOW)\n            pred = model.predict(Xf.reshape(-1,WINDOW,1), verbose=0).flatten()[-horizon:]\n        else:  # NBeats\n            Xtr, Ytr = make_seq(train.values, WINDOW)\n            class Block(layers.Layer):\n                def __init__(self):\n                    super().__init__()\n                    self.hidden = [layers.Dense(params['units'], activation='relu') for _ in range(4)]\n                    self.backcast = layers.Dense(WINDOW)\n                    self.forecast = layers.Dense(1)\n                def call(self, x):\n                    h = x\n                    for lyr in self.hidden:\n                        h = lyr(h)\n                    return self.backcast(h), self.forecast(h)\n            inp = layers.Input(shape=(WINDOW,))\n            res = inp\n            fc_sum = 0\n            for _ in range(params['stacks']):\n                b, f = Block()(res)\n                res = res - b\n                fc_sum = fc_sum + f\n            nb = models.Model(inp, fc_sum)\n            nb.compile('adam','mse')\n            nb.fit(\n                Xtr, Ytr,\n                epochs=params['epochs'], batch_size=32,\n                verbose=0\n            )\n            seq = np.concatenate([train.values[-WINDOW:], test.values])\n            Xf, _ = make_seq(seq, WINDOW)\n            pred = nb.predict(Xf, verbose=0).flatten()[-horizon:]\n        # Plot\n        ax.plot(train.index, train.values, label='Train')\n        ax.plot(test.index, test.values, label='Real')\n        ax.plot(test.index, pred, label='Predicción')\n        ax.set_title(f\"{muni} - {serv}\")\n        ax.legend()\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T04:21:46.565625Z","iopub.execute_input":"2025-04-25T04:21:46.565991Z","iopub.status.idle":"2025-04-25T04:21:46.595530Z","shell.execute_reply.started":"2025-04-25T04:21:46.565962Z","shell.execute_reply":"2025-04-25T04:21:46.594323Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-1c1f740b898f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mforecasting_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_forecasts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mDibuja\u001b[0m \u001b[0mcomparación\u001b[0m \u001b[0mde\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSMAPE\u001b[0m \u001b[0my\u001b[0m \u001b[0mMASE\u001b[0m \u001b[0mpor\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'forecasting_pipeline'"],"ename":"ModuleNotFoundError","evalue":"No module named 'forecasting_pipeline'","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"plot_forecasts(df, combos, best_model, params_dict[best_model])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T04:19:14.692146Z","iopub.execute_input":"2025-04-25T04:19:14.692479Z","iopub.status.idle":"2025-04-25T04:19:14.711292Z","shell.execute_reply.started":"2025-04-25T04:19:14.692449Z","shell.execute_reply":"2025-04-25T04:19:14.710202Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-21aef9047e19>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from forecasting_pipeline import df, combos, best_model, best_params, plot_forecasts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_forecasts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'plot_forecasts' is not defined"],"ename":"NameError","evalue":"name 'plot_forecasts' is not defined","output_type":"error"}],"execution_count":14}]}